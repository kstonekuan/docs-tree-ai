# DocTreeAI Environment Configuration
# Copy this file to .env and modify the values for your setup

# =============================================================================
# REQUIRED CONFIGURATION
# =============================================================================

# Local LLM Server Configuration
# The base URL of your local LLM server (must be OpenAI-compatible)
# Examples:
#   - Ollama: http://localhost:11434/v1
#   - LM Studio: http://localhost:1234/v1
#   - vLLM: http://localhost:8000/v1
OPENAI_API_BASE=http://localhost:11434/v1

# The name/identifier of the model to use for summarization
# STRONGLY RECOMMENDED: OpenAI's GPT-OSS-20B for optimal documentation generation
# - Excellent code analysis and documentation capabilities
# - Runs efficiently on consumer GPUs (16GB)
# - Superior reasoning and technical writing quality
OPENAI_MODEL_NAME=gpt-oss:20b

# =============================================================================
# OPTIONAL CONFIGURATION
# =============================================================================

# API Key for authentication (can be placeholder for local models)
# For most local LLM servers, this can be any string or left as "local"
OPENAI_API_KEY=local

# Alternative environment variable names (supported for compatibility)
# You can use these instead of the OPENAI_* variants above:
# OPENAI_BASE_URL=http://localhost:11434/v1
# OPENAI_MODEL=gpt-oss:20b

# Custom cache directory name (default: .doctreeai_cache)
# This directory will be created in your project root and added to .gitignore
DOCTREEAI_CACHE_DIR=.doctreeai_cache

# Logging configuration
# Options: error, warn, info, debug, trace
# Default: info
DOCTREEAI_LOG_LEVEL=info

# Alternative logging configuration
# LOG_LEVEL=debug

# =============================================================================
# USAGE NOTES
# =============================================================================

# 1. Copy this file: cp .env.example .env
# 2. Edit .env with your specific configuration
# 3. Make sure your LLM server is running before using DocTreeAI
# 4. Test your setup with: doctreeai test
# 5. The .env file is automatically ignored by git (.gitignore)

# Common LLM Servers Setup:
# 
# Ollama (RECOMMENDED for GPT-OSS-20B):
#   - Install: https://ollama.com/
#   - Pull GPT-OSS-20B: ollama pull gpt-oss:20b
#   - Start server: ollama serve
#   - Default: http://localhost:11434
#
# LM Studio (Alternative for GPT-OSS-20B):
#   - Download: https://lmstudio.ai/
#   - Search and download: openai/gpt-oss-20b
#   - Load the model and start local server
#   - Default: http://localhost:1234

# Performance Tips:
# - GPT-OSS-20B offers the best balance of quality and performance for documentation
# - Only requires 16GB GPU memory with quantization
# - Provides superior code analysis compared to general-purpose models
# - Use GPU acceleration if available for optimal performance
# - Consider context window size for very large codebases